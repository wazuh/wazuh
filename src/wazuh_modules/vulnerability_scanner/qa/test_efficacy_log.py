import json
import subprocess
import os
import time
import re
from pathlib import Path
import glob
import pytest
from jsonschema import validate
from jsonschema.exceptions import ValidationError
import logging
import requests
import docker

LOGGER = logging.getLogger(__name__)
OPENSEARCH_IP = "127.0.0.1:9200"

def init_opensearch(low_resources):
    client = docker.from_env()
    env_vars = {
            'discovery.type': 'single-node',
            'plugins.security.disabled': 'true',
            'OPENSEARCH_INITIAL_ADMIN_PASSWORD': 'WazuhTest99$'
        }

    if low_resources:
        env_vars['http.max_content_length'] = '4mb'

    # Check if container already exists
    try:
        existing = client.containers.get('opensearch')
        LOGGER.info("OpenSearch container already exists, reusing it")
        if existing.status != 'running':
            existing.start()
    except docker.errors.NotFound:
        # Container doesn't exist, create it
        client.containers.run("opensearchproject/opensearch", detach=True, ports={'9200/tcp': 9200},
                              environment=env_vars, name='opensearch', stdout=True, stderr=True)

    ## Wait for the container is running and opensearch is ready
    while True:
        try:
            response = requests.get('http://'+OPENSEARCH_IP+'')
            if response.status_code == 200:
                break
        except requests.exceptions.ConnectionError:
            pass
        time.sleep(1)
    return client


@pytest.fixture(scope='module')
def opensearch(request):
    low_resources = request.param
    client = init_opensearch(low_resources)
    yield client
    # Stop all containers
    for container in client.containers.list():
        container.stop()
    client.containers.prune()


def opensearch_cleanup():
    """Delete all data from OpenSearch indices before each test."""
    indices = ['wazuh-states-inventory-system',
               'wazuh-states-inventory-packages',
               'wazuh-states-vulnerabilities']
    for index in indices:
        url = f'http://{OPENSEARCH_IP}/{index}/_delete_by_query'
        query = {"query": {"match_all": {}}}
        try:
            response = requests.post(url, json=query, headers={'Content-Type': 'application/json'})
            if response.status_code not in [200, 404]:  # 404 OK if index doesn't exist
                LOGGER.warning(f"Failed to clean {index}: {response.status_code} {response.text}")
            else:
                LOGGER.info(f"Cleaned OpenSearch index: {index}")
        except Exception as e:
            LOGGER.warning(f"Error cleaning {index}: {e}")


@pytest.mark.parametrize('opensearch', [False], indirect=True)
def test_opensearch_health(opensearch):
    url = 'http://'+OPENSEARCH_IP+'/_cluster/health'
    response = requests.get(url)
    assert response.status_code == 200
    assert response.json()['status'] == 'green' or response.json()['status'] == 'yellow'


def find_regex_in_file(regex, file, times=1, max_timeout=3):
    pattern = re.compile(regex)
    start_time = time.time()

    while time.time() - start_time < max_timeout:
        count = 0
        with open(file, 'r') as f:
            content = f.read()
            count = len(pattern.findall(content))
            LOGGER.debug(f"Found {count} matches")
            if count == times:
                return True

        time.sleep(0.5)
    return False


def tail_log(file, expected_lines, found_lines, timeout):
    start_time = time.time()
    with open(file, "r") as f:
        while not all(found_lines.values()) and (time.time() - start_time <= timeout):
            line = f.readline()
            if not line:
                continue
            # Check if the line contains the expected output
            for expected in expected_lines:
                if expected in line and not found_lines[expected]:
                    LOGGER.info(f"Found log line: {line}")
                    found_lines[expected] = True


@pytest.fixture
def run_on_end(request):
    yield
    # Read the location of the log
    if 'GITHUB_WORKSPACE' not in os.environ:
        LOGGER.info("GITHUB_WORKSPACE is not defined")
        return
    path = os.environ['GITHUB_WORKSPACE']
    # Search for the log.out file in path variable
    for file_path in glob.glob(f'{path}/**/log.out', recursive=True):
        # Copy the file found to another directory
        LOGGER.info(f"Copying {file_path} to {path}/qa_logs/log.out.{request.node.name}")
        os.system(f"cp {file_path} {path}/qa_logs/log.out.{request.node.name}")


@pytest.fixture
def run_process_and_monitor_log(request, run_on_end):
    # Clean OpenSearch data from previous tests
    opensearch_cleanup()

    # Delete previous inventory_sync directory if exists
    if Path("inventory_sync").exists():
        for file in Path("inventory_sync").glob("*"):
            file.unlink()
        Path("inventory_sync").rmdir()

    # Set the path to the binary (centralized build)
    cmd = Path("build/bin/", "inventory_sync_testtool")

    # Ensure the binary exists
    assert cmd.exists(), "The binary does not exist at {}".format(cmd)

    test_folder = request.param
    LOGGER.debug(f"Running test {test_folder}")

    # Remove previous log file if exists
    log_file = "log.out"
    if Path(log_file).exists():
        Path(log_file).unlink()

    # Find input JSON files in the test directory
    json_files = sorted(Path(test_folder).glob("input_*.json"))
    assert len(json_files) > 0, f"No input_*.json files found in {test_folder}"

    # Load expected output
    expected_json_files = sorted(Path(test_folder).glob("expected_*.json"))
    expected_lines = []
    for expected_json_file in expected_json_files:
        # Parse text file with expected log entries (format: function message)
        with open(expected_json_file, 'r') as f:
            for line in f:
                line = line.strip()
                # Skip empty lines and brackets
                if line and line not in ['[', ']']:
                    # Remove quotes and trailing comma if present
                    line = line.strip('"').rstrip(',').strip('"')
                    expected_lines.append(line)

    LOGGER.debug(f"Expected lines: {expected_lines}")
    found_lines = {line: False for line in expected_lines}

    # Run the testtool for each input file
    for json_file in json_files:
        LOGGER.info(f"Running test with input: {json_file}")

        # Build command: ./inventory_sync_testtool <input.json> --config <config.json> --logFile <log.out>
        config_file = Path("wazuh_modules/vulnerability_scanner/qa/test_data/config.json")
        command = [
            str(cmd),
            str(json_file.resolve()),
            "--config", str(config_file.resolve()),
            "--logFile", log_file,
        ]

        LOGGER.debug(f"Executing: {' '.join(command)}")

        # Run the process and wait for completion
        result = subprocess.run(command, capture_output=True, text=True, timeout=120)

        LOGGER.debug(f"Process exit code: {result.returncode}")
        if result.stdout:
            LOGGER.debug(f"stdout: {result.stdout}")
        if result.stderr:
            LOGGER.debug(f"stderr: {result.stderr}")

        # Check if the log file was created
        assert Path(log_file).exists(), f"Log file {log_file} was not created"

    # Parse the log file and check for expected lines
    LOGGER.info(f"Checking log file for expected output")
    with open(log_file, 'r') as f:
        for log_line in f:
            log_line = log_line.strip()

            # Check against expected lines (direct match since format is: function():message)
            for expected_line in expected_lines:
                if expected_line in log_line or log_line == expected_line:
                    if not found_lines[expected_line]:
                        found_lines[expected_line] = True
                        LOGGER.info(f"✓ Found: {expected_line[:80]}")

    # Report missing lines AFTER checking all log lines
    for expected_line, found in found_lines.items():
        if not found:
            LOGGER.error(f"✗ Missing: {expected_line[:80]}")

    return found_lines

test_folders = [f for f in sorted(Path("wazuh_modules/vulnerability_scanner/qa/test_data").glob(os.getenv('WAZUH_VD_TEST_GLOB', '*'))) if f.is_dir()]

@pytest.mark.parametrize("run_process_and_monitor_log", test_folders, indirect=True)
def test_data(run_process_and_monitor_log):
    # change working directory to the root of the project parent directory
    # This is required to run the binary
    os.chdir(Path(__file__).parent.parent.parent.parent)

    found_lines = run_process_and_monitor_log
    for line, found in found_lines.items():
        if not found:
            LOGGER.error(f"Log entry not found: {line}")
    assert all(found_lines.values()), "The test failed because some expected lines were not found"
