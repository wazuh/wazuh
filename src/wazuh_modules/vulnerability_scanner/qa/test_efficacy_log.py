import json
import platform
import subprocess
import socket
import os
import time
import threading
from pathlib import Path
import glob

import pytest
from jsonschema import validate
from jsonschema.exceptions import ValidationError

import logging

LOGGER = logging.getLogger(__name__)
import re
import time

isDeltas = False
isRsync = False

def find_regex_in_file(regex, file, times=1, max_timeout=3):
    pattern = re.compile(regex)
    start_time = time.time()

    while time.time() - start_time < max_timeout:
        count = 0
        with open(file, 'r') as f:
            content = f.read()
            count = len(pattern.findall(content))
            LOGGER.debug(f"Found {count} matches")
            if count == times:
                return True

        time.sleep(0.5)
    return False

def sendflatbuffer_to_unixsocket(data):
    # Create a unix socket
    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    # Connect to the socket
    try:
        # Check if the socket exists
        if isDeltas == True:
            if not Path("queue/router/deltas-syscollector").exists():
                LOGGER.info(f"Socket does not exists")
                return None
            sock.connect("queue/router/deltas-syscollector")
        elif isRsync == True:
            if not Path("queue/router/rsync").exists():
                LOGGER.info(f"Socket does not exists")
                return None
            sock.connect("queue/router/rsync")
        else:
            LOGGER.info(f"Socket does not exists")
            return None
        size = len(data)+5
        data_to_send = size.to_bytes(4, byteorder="little")
        header_size = 1
        data_to_send += header_size.to_bytes(4, byteorder="little")
        data_to_send += b"P"
        data_to_send += data

        # Send the data
        sock.send(data_to_send)

    except Exception as e:
        LOGGER.info(f"Socket error {e}")
        return None


def json2binary(test, output):
    command = ["external/flatbuffers/build/flatc", "--binary", "-o", output, "shared_modules/utils/flatbuffers/schemas/syscollector_deltas.fbs", test]
    # Execute the flatbuffer compiler
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

    global isDeltas
    global isRsync

    # if the result is not 0, test with another fbs schema.
    if result.returncode != 0:
        command = ["external/flatbuffers/build/flatc", "--binary", "-o", output, "shared_modules/utils/flatbuffers/schemas/rsync.fbs", test]
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        if result.returncode == 0:
            isRsync = True
            isDeltas = False
            LOGGER.debug(f"Is rsync")
    else:
        isDeltas = True
        isRsync = False
        LOGGER.debug(f"Is deltas")

    # if the result is not 0, stop the test
    assert result.returncode == 0, f"Error: {result.stdout}"

def tail_log(file, expected_lines, found_lines, timeout):
    start_time = time.time()
    with open(file, "r") as f:
        while not all(found_lines.values()) and (time.time() - start_time <= timeout):
            line = f.readline()
            if not line:
                continue
            # Check if the line contains the expected output
            for expected in expected_lines:
                if expected in line and not found_lines[expected]:
                    LOGGER.info(f"Found log line: {line}")
                    found_lines[expected] = True

@pytest.fixture
def run_on_end(request):
    yield
    # Read the location of the log
    if 'GITHUB_WORKSPACE' not in os.environ:
        LOGGER.info("GITHUB_WORKSPACE is not defined")
        return
    path = os.environ['GITHUB_WORKSPACE']
    # Search for the log.out file in path variable
    for file_path in glob.glob(f'{path}/**/log.out', recursive=True):
        # Copy the file found to another directory
        LOGGER.info(f"Copying {file_path} to {path}/qa_logs/log.out.{request.node.name}")
        os.system(f"cp {file_path} {path}/qa_logs/log.out.{request.node.name}")

@pytest.fixture
def run_process_and_monitor_log(request, run_on_end):
    # Delete previous inventory directory if exists
    if Path("queue/vd/inventory").exists():
        for file in Path("queue/vd/inventory").glob("*"):
            file.unlink()
        Path("queue/vd/inventory").rmdir()

    # Set the path to the binary
    cmd = Path("build/wazuh_modules/vulnerability_scanner/testtool/scanner/", "vd_scanner_testtool")
    cmdAlt = Path("wazuh_modules/vulnerability_scanner/build/testtool/scanner/", "vd_scanner_testtool")

    # Ensure the binary exists
    if not cmd.exists():
        cmd = cmdAlt
    assert cmd.exists(), "The binary does not exists"

    args = ["-c", "wazuh_modules/vulnerability_scanner/testtool/scanner/config.json",
            "-t", "wazuh_modules/vulnerability_scanner/indexer/template/index-template.json",
            "-l", "log.out",
            "-s", "120",
            "-h", "wazuh_modules/vulnerability_scanner/testtool/scanner/fakeAgentData/agentHotfixesData.json",
            "-b", "wazuh_modules/vulnerability_scanner/testtool/scanner/fakeGlobalData/globalData.json",
            "-u"]

    command = [cmd] + args
    test_folder = request.param

    LOGGER.debug(f"Running test {test_folder}")

    # Remove previous log file if exists
    if Path("log.out").exists():
        Path("log.out").unlink()

    # Iterate over json files in the test directory, convert to flatbuffer and send through unix socket
    with subprocess.Popen(command) as process:
        # Check if the process is initialized finding in the log file the line "Vulnerability scanner module started"
        start_time = time.time()
        log_file = "log.out"

        # Wait until module starts
        while not Path(log_file).exists() and (time.time() - start_time <= 10):
            time.sleep(1)

        # Check if the log file exists, if the line is not found, try again in 1 second
        assert Path(log_file).exists(), "The log file does not exists"

        LOGGER.debug(f"Wating for the process to be initialized")
        found = find_regex_in_file(r"Vulnerability scanner module started", log_file)
        assert found, "The process is not initialized, timeout waiting vulnerability scanner module to start."
        LOGGER.info(f"Process initialized")

        expected_json_files = sorted(Path(test_folder).glob("expected_*.out"))
        expected_lines = []
        # Read expected output if it exists, this is an json with and array of lines.
        for expected_json_file in expected_json_files:
            # Parse json and add the string elements of te array to the expected lines
            json_data = json.load(open(expected_json_file))
            for line in json_data:
                expected_lines.append(line)

        LOGGER.debug(f"Expected lines: {expected_lines}")
        quantity_expected_lines = len(expected_lines)
        LOGGER.debug(f"Quantity expected lines: {quantity_expected_lines}")

        found_lines = {line: False for line in expected_lines}

        json_files = sorted(Path(test_folder).glob("input_*.json"))

        for json_file in json_files:
            LOGGER.debug(f"Running test {json_file}")
            with open(json_file) as f:
                # Set the output file
                file = str(json_file)

                # Parse json file and print the data
                json_data = json.load(open(file))

                # Set the output folder
                output_folder = str(test_folder)

                # Set the output file
                output = str(json_file).replace(".json", ".bin")

                # Convert the json data to flatbuffer
                json2binary(file, output_folder)

                # Read the flatbuffer data
                with open(output, "rb") as f:
                    flatbuffer_data = f.read()

                LOGGER.debug("Sending flatbuffer data")

                # After start to read lines, send the flatbuffer data
                sendflatbuffer_to_unixsocket(flatbuffer_data)

        # Wait until the scan is finished.
        regex = r"Event type: (.*) processed"
        found = find_regex_in_file(regex, log_file, len(expected_json_files))

        if not found:
            LOGGER.info("The scan is not finished, some events were not processed")
            process.terminate()
            return []

        LOGGER.info(f"Scan finished, all events were processed")

        timeout = 1
        basetimeout = timeout
        for expected_line in expected_lines:
            while not found_lines[expected_line]:
                LOGGER.debug(f"Waiting for log line: {expected_line}")
                if timeout < 10*basetimeout:
                    tail_log(log_file, expected_lines, found_lines, timeout)
                    timeout = 2*timeout
                else:
                    LOGGER.error(f"Timeout waiting for log line: {expected_line}")
                    break

        process.terminate()

    LOGGER.debug(f"Waiting for the process to finish")
    return found_lines

test_folders = sorted(Path("wazuh_modules/vulnerability_scanner/qa/test_data").glob(os.getenv('WAZUH_VD_TEST_GLOB', '*')))

@pytest.mark.parametrize("run_process_and_monitor_log", test_folders, indirect=True)
def test_data(run_process_and_monitor_log):
    # change working directory to the root of the project parent directory
    # This is required to run the binary
    os.chdir(Path(__file__).parent.parent.parent.parent)

    found_lines = run_process_and_monitor_log
    for line, found in found_lines.items():
        if not found:
            LOGGER.error(f"Log entry not found: {line}")
    assert all(found_lines.values()), "The test failed because some expected lines were not found"
